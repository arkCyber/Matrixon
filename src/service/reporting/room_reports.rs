// =============================================================================
// Matrixon Matrix NextServer - Room Reports Module
// =============================================================================
//
// Project: Matrixon - Ultra High Performance Matrix NextServer (Synapse Alternative)
// Author: arkSong (arksong2018@gmail.com) - Founder of Matrixon Innovation Project
// Contributors: Matrixon Development Team
// Date: 2024-12-11
// Version: 2.0.0-alpha (PostgreSQL Backend)
// License: Apache 2.0 / MIT
//
// Description:
//   Core business logic service implementation. This module is part of the Matrixon Matrix NextServer
//   implementation, designed for enterprise-grade deployment with 20,000+
//   concurrent connections and <50ms response latency.
//
// Performance Targets:
//   â€¢ 20k+ concurrent connections
//   â€¢ <50ms response latency
//   â€¢ >99% success rate
//   â€¢ Memory-efficient operation
//   â€¢ Horizontal scalability
//
// Features:
//   â€¢ Business logic implementation
//   â€¢ Service orchestration
//   â€¢ Event handling and processing
//   â€¢ State management
//   â€¢ Enterprise-grade reliability
//
// Architecture:
//   â€¢ Async/await native implementation
//   â€¢ Zero-copy operations where possible
//   â€¢ Memory pool optimization
//   â€¢ Lock-free data structures
//   â€¢ Enterprise monitoring integration
//
// Dependencies:
//   â€¢ Tokio async runtime
//   â€¢ Structured logging with tracing
//   â€¢ Error handling with anyhow/thiserror
//   â€¢ Serialization with serde
//   â€¢ Matrix protocol types with ruma
//
// References:
//   â€¢ Matrix.org specification: https://matrix.org/
//   â€¢ Synapse reference: https://github.com/element-hq/synapse
//   â€¢ Matrix spec: https://spec.matrix.org/
//   â€¢ Performance guidelines: Internal Matrixon documentation
//
// Quality Assurance:
//   â€¢ Comprehensive unit testing
//   â€¢ Integration test coverage
//   â€¢ Performance benchmarking
//   â€¢ Memory leak detection
//   â€¢ Security audit compliance
//
// =============================================================================

use std::{
    collections::HashMap,
    sync::Arc,
    time::{Duration, SystemTime},
};

use ruma::{
    api::client::error::ErrorKind,
    events::AnyTimelineEvent,
    events::{
        room::{
            message::MessageType,
        },
        StateEventType,
    },
    OwnedRoomId, RoomId, OwnedUserId, UserId, OwnedEventId, EventId,
};

use serde::{Deserialize, Serialize};
use tokio::sync::RwLock;
use tracing::{debug, info, instrument};

use crate::{
    service::Services,
    Error, Result,
};
use base64::{engine::general_purpose, Engine as _};
use crate::utils;
use crate::services;

/// Room reporting service implementing MSC4151
#[derive(Debug)]
pub struct RoomReportingService {
    /// Active room reports
    reports: Arc<RwLock<HashMap<String, RoomReport>>>,
    
    /// Room content filters
    content_filters: Arc<RwLock<Vec<ContentFilter>>>,
    
    /// Room moderation policies
    moderation_policies: Arc<RwLock<HashMap<OwnedRoomId, RoomModerationPolicy>>>,
    
    /// Content analysis cache
    analysis_cache: Arc<RwLock<HashMap<String, ContentAnalysis>>>,
    
    /// Configuration
    config: RoomReportingConfig,
    
    /// Metrics collector
    metrics: Arc<RoomReportingMetrics>,
}

/// Room report structure
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RoomReport {
    /// Report ID
    pub report_id: String,
    
    /// Reporter user ID
    pub reporter_user_id: OwnedUserId,
    
    /// Reported room ID
    pub room_id: OwnedRoomId,
    
    /// Specific event ID (if applicable)
    pub event_id: Option<OwnedEventId>,
    
    /// Report category
    pub category: RoomReportCategory,
    
    /// Report reason
    pub reason: String,
    
    /// Additional details
    pub details: Option<String>,
    
    /// Report timestamp
    pub created_at: SystemTime,
    
    /// Report status
    pub status: ReportStatus,
    
    /// Processing notes
    pub processing_notes: Vec<ProcessingNote>,
    
    /// Content evidence
    pub evidence: Vec<ContentEvidence>,
    
    /// Severity level
    pub severity: SeverityLevel,
    
    /// Auto-detected issues
    pub auto_detected_issues: Vec<ContentIssue>,
    
    /// Reporter's role in room
    pub reporter_role: RoomRole,
}

/// Room report categories
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum RoomReportCategory {
    /// Inappropriate content
    InappropriateContent,
    
    /// Spam or excessive posting
    Spam,
    
    /// Harassment in room
    Harassment,
    
    /// Hate speech or discrimination
    HateSpeech,
    
    /// Illegal content sharing
    IllegalContent,
    
    /// Room impersonation
    Impersonation,
    
    /// Privacy violations
    PrivacyViolation,
    
    /// Copyright infringement
    Copyright,
    
    /// Room used for illegal activities
    IllegalActivity,
    
    /// Malicious room (phishing, malware)
    MaliciousContent,
    
    /// Off-topic or misuse of room purpose
    OffTopic,
    
    /// Other policy violation
    Other,
}

/// Report processing status
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum ReportStatus {
    /// Report submitted
    Submitted,
    
    /// Content under review
    UnderReview,
    
    /// Escalated for manual review
    Escalated,
    
    /// Content removed
    ContentRemoved,
    
    /// Room quarantined
    RoomQuarantined,
    
    /// No action taken
    NoAction,
    
    /// Report dismissed
    Dismissed,
    
    /// Duplicate report
    Duplicate,
}

/// User's role in room context
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum RoomRole {
    /// Room creator
    Creator,
    
    /// Room administrator
    Admin,
    
    /// Room moderator
    Moderator,
    
    /// Regular member
    Member,
    
    /// Invited user
    Invited,
    
    /// Banned user
    Banned,
    
    /// External observer
    External,
}

/// Content evidence item
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContentEvidence {
    /// Evidence type
    pub evidence_type: ContentEvidenceType,
    
    /// Evidence content
    pub content: String,
    
    /// Related event ID
    pub event_id: Option<OwnedEventId>,
    
    /// Timestamp when evidence was collected
    pub collected_at: SystemTime,
    
    /// Content hash for integrity
    pub content_hash: String,
    
    /// Metadata
    pub metadata: HashMap<String, serde_json::Value>,
}

/// Content evidence types
#[derive(Debug, Clone, Serialize, Deserialize)]
#[derive(PartialEq, Hash)]
pub enum ContentEvidenceType {
    /// Message content
    MessageContent,
    
    /// Room metadata (name, topic, avatar)
    RoomMetadata,
    
    /// Media content
    MediaContent,
    
    /// State events
    StateEvents,
    
    /// Room history snapshot
    RoomHistory,
    
    /// Member list
    MemberList,
}

/// Auto-detected content issue
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContentIssue {
    /// Issue type
    pub issue_type: String,
    
    /// Confidence score
    pub confidence: f64,
    
    /// Detection timestamp
    pub detected_at: SystemTime,
    
    /// Issue details
    pub details: HashMap<String, serde_json::Value>,
    
    /// Suggested action
    pub suggested_action: Option<String>,
}

/// Content filter rule
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContentFilter {
    /// Filter ID
    pub filter_id: String,
    
    /// Filter name
    pub name: String,
    
    /// Filter type
    pub filter_type: FilterType,
    
    /// Filter patterns
    pub patterns: Vec<String>,
    
    /// Filter actions
    pub actions: Vec<FilterAction>,
    
    /// Whether filter is enabled
    pub enabled: bool,
    
    /// Priority level
    pub priority: u32,
}

/// Content filter types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum FilterType {
    /// Keyword matching
    KeywordFilter,
    
    /// Regular expression
    RegexFilter,
    
    /// Image analysis
    ImageFilter,
    
    /// Link analysis
    LinkFilter,
    
    /// Behavioral pattern
    BehaviorFilter,
    
    /// ML-based classification
    MLFilter,
}

/// Filter action
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FilterAction {
    /// Action type
    pub action_type: FilterActionType,
    
    /// Action parameters
    pub parameters: HashMap<String, serde_json::Value>,
}

/// Filter action types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum FilterActionType {
    /// Flag content for review
    FlagForReview,
    
    /// Automatically remove content
    AutoRemove,
    
    /// Quarantine media
    QuarantineMedia,
    
    /// Send warning to user
    WarnUser,
    
    /// Rate limit user
    RateLimitUser,
    
    /// Create report
    CreateReport,
    
    /// Log incident
    LogIncident,
}

/// Room moderation policy
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RoomModerationPolicy {
    /// Room ID
    pub room_id: OwnedRoomId,
    
    /// Content restrictions
    pub content_restrictions: ContentRestrictions,
    
    /// Auto-moderation settings
    pub auto_moderation: AutoModerationSettings,
    
    /// Reporting settings
    pub reporting_settings: ReportingSettings,
    
    /// Policy version
    pub version: u32,
    
    /// Last updated
    pub updated_at: SystemTime,
}

/// Content restrictions
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContentRestrictions {
    /// Allow media uploads
    pub allow_media: bool,
    
    /// Allow external links
    pub allow_links: bool,
    
    /// Maximum message length
    pub max_message_length: Option<u32>,
    
    /// Allowed file types
    pub allowed_file_types: Vec<String>,
    
    /// Blocked keywords
    pub blocked_keywords: Vec<String>,
    
    /// Require content approval
    pub require_approval: bool,
}

/// Auto-moderation settings
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AutoModerationSettings {
    /// Enable auto-moderation
    pub enabled: bool,
    
    /// Auto-remove threshold
    pub auto_remove_threshold: f64,
    
    /// Auto-quarantine threshold
    pub auto_quarantine_threshold: f64,
    
    /// Enable spam detection
    pub spam_detection: bool,
    
    /// Enable content filtering
    pub content_filtering: bool,
}

/// Reporting settings
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ReportingSettings {
    /// Allow member reports
    pub allow_member_reports: bool,
    
    /// Allow external reports
    pub allow_external_reports: bool,
    
    /// Auto-escalate threshold
    pub auto_escalate_threshold: u32,
    
    /// Required reporter trust score
    pub min_reporter_trust_score: f64,
}

/// Content analysis result
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContentAnalysis {
    /// Content ID
    pub content_id: String,
    
    /// Analysis timestamp
    pub analyzed_at: SystemTime,
    
    /// Content type
    pub content_type: String,
    
    /// Risk scores
    pub risk_scores: RiskScores,
    
    /// Detected issues
    pub detected_issues: Vec<ContentIssue>,
    
    /// Recommended actions
    pub recommended_actions: Vec<String>,
}

/// Risk assessment scores
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RiskScores {
    /// Overall risk score (0.0 - 1.0)
    pub overall: f64,
    
    /// Toxicity score
    pub toxicity: f64,
    
    /// Spam score
    pub spam: f64,
    
    /// Inappropriate content score
    pub inappropriate: f64,
    
    /// Violence score
    pub violence: f64,
    
    /// Illegal content score
    pub illegal: f64,
}

/// Severity levels
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, PartialOrd, Ord)]
pub enum SeverityLevel {
    Low = 1,
    Medium = 2,
    High = 3,
    Critical = 4,
}

/// Processing note
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessingNote {
    /// Note timestamp
    pub timestamp: SystemTime,
    
    /// Moderator who added the note
    pub moderator: OwnedUserId,
    
    /// Note content
    pub note: String,
    
    /// Note type
    pub note_type: NoteType,
}

/// Note types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum NoteType {
    /// General comment
    Comment,
    
    /// Action taken
    Action,
    
    /// Status update
    StatusUpdate,
    
    /// Escalation
    Escalation,
}

/// Configuration for room reporting
#[derive(Debug, Clone)]
pub struct RoomReportingConfig {
    /// Enable room reporting
    pub enabled: bool,
    
    /// Auto-processing enabled
    pub auto_processing: bool,
    
    /// Maximum reports per user per day
    pub max_reports_per_day: u32,
    
    /// Content analysis enabled
    pub content_analysis: bool,
    
    /// Auto-moderation enabled
    pub auto_moderation: bool,
    
    /// Evidence retention period
    pub evidence_retention_days: u32,
    
    /// Enable real-time monitoring
    pub realtime_monitoring: bool,
}

/// Metrics for room reporting
#[derive(Debug, Default)]
pub struct RoomReportingMetrics {
    /// Total room reports submitted
    pub total_reports: std::sync::atomic::AtomicU64,
    
    /// Reports by category
    pub reports_by_category: Arc<RwLock<HashMap<RoomReportCategory, u64>>>,
    
    /// Content removed
    pub content_removed: std::sync::atomic::AtomicU64,
    
    /// Rooms quarantined
    pub rooms_quarantined: std::sync::atomic::AtomicU64,
    
    /// False positive rate
    pub false_positive_rate: std::sync::atomic::AtomicU64,
    
    /// Average processing time
    pub avg_processing_time: std::sync::atomic::AtomicU64,
}

impl Default for RoomReportingConfig {
    fn default() -> Self {
        Self {
            enabled: true,
            auto_processing: true,
            max_reports_per_day: 20,
            content_analysis: true,
            auto_moderation: false, // Disabled by default for safety
            evidence_retention_days: 90,
            realtime_monitoring: true,
        }
    }
}

impl RoomReportingService {
    /// Create new room reporting service
    pub fn new() -> Self {
        Self {
            reports: Arc::new(RwLock::new(HashMap::new())),
            content_filters: Arc::new(RwLock::new(Vec::new())),
            moderation_policies: Arc::new(RwLock::new(HashMap::new())),
            analysis_cache: Arc::new(RwLock::new(HashMap::new())),
            config: RoomReportingConfig::default(),
            metrics: Arc::new(RoomReportingMetrics::default()),
        }
    }

    /// Create with custom configuration
    pub fn with_config(config: RoomReportingConfig) -> Self {
        Self {
            reports: Arc::new(RwLock::new(HashMap::new())),
            content_filters: Arc::new(RwLock::new(Vec::new())),
            moderation_policies: Arc::new(RwLock::new(HashMap::new())),
            analysis_cache: Arc::new(RwLock::new(HashMap::new())),
            config,
            metrics: Arc::new(RoomReportingMetrics::default()),
        }
    }

    // ========== MSC4151 Room Reporting API ==========

    /// Submit room report (MSC4151)
    /// POST /_matrix/client/v1/report/room/{room_id}
    #[instrument(level = "debug", skip(self))]
    pub async fn submit_room_report(
        &self,
        reporter: &UserId,
        _room_id: &RoomId,
        _event_id: Option<&EventId>,
        category: RoomReportCategory,
        reason: String,
        details: Option<String>,
    ) -> Result<String> {
        let start_time = SystemTime::now();
        debug!("ðŸš¨ Submitting room report for {}", _room_id);

        // Validate reporter and room access
        self.validate_room_reporter(reporter, _room_id).await?;

        // Check rate limits
        self.check_room_reporting_rate_limits(reporter).await?;

        // Generate report ID
        let report_id = utils::random_string(16);

        // Determine reporter's role in room
        let reporter_role = self.get_user_room_role(reporter, _room_id).await?;

        // Collect content evidence
        let evidence = self.collect_room_evidence(
            _room_id,
            _event_id,
            &category,
        ).await?.to_vec();

        // Run content analysis
        let auto_detected_issues = if self.config.content_analysis {
            self.analyze_room_content(_room_id, _event_id, &evidence).await?
        } else {
            Vec::new()
        };

        // Determine severity
        let severity = self.calculate_room_report_severity(&category, &auto_detected_issues).await;

        // Create report
        let report = RoomReport {
            report_id: report_id.clone(),
            reporter_user_id: reporter.to_owned(),
            room_id: _room_id.to_owned(),
            event_id: _event_id.map(|e| e.to_owned()),
            category,
            reason,
            details,
            created_at: SystemTime::now(),
            status: ReportStatus::Submitted,
            processing_notes: Vec::new(),
            evidence,
            severity,
            auto_detected_issues,
            reporter_role,
        };

        // Store report
        self.reports.write().await.insert(report_id.clone(), report);

        // Update metrics
        self.metrics.total_reports.fetch_add(1, std::sync::atomic::Ordering::Relaxed);

        // Trigger auto-processing if enabled
        if self.config.auto_processing {
            self.process_room_report_auto(&report_id).await?;
        }

        info!("âœ… Room report submitted: {} in {:?}", 
              report_id, start_time.elapsed().unwrap_or_default());

        Ok(report_id)
    }

    /// Get room report status (MSC4151)
    /// GET /_matrix/client/v1/report/room/{report_id}
    #[instrument(level = "debug", skip(self))]
    pub async fn get_room_report_status(
        &self,
        requester: &UserId,
        report_id: &str,
    ) -> Result<RoomReportStatus> {
        debug!("ðŸ“‹ Getting room report status: {}", report_id);

        let reports = self.reports.read().await;
        let report = reports.get(report_id).ok_or_else(|| Error::BadRequestString(
            ErrorKind::NotFound,
            "Report not found".to_string(),
        ))?;

        // Check permissions
        if report.reporter_user_id != requester && !services().users.is_admin(requester)? {
            return Err(Error::BadRequestString(
                ErrorKind::forbidden(),
                "Access denied to this report".to_string(),
            ));
        }

        Ok(RoomReportStatus {
            report_id: report.report_id.clone(),
            room_id: report.room_id.clone(),
            status: report.status.clone(),
            created_at: report.created_at,
            last_updated: report.processing_notes.last()
                .map(|note| note.timestamp)
                .unwrap_or(report.created_at),
            category: report.category.clone(),
            severity: report.severity.clone(),
        })
    }

    /// List room reports (Admin API)
    /// GET /_matrix/client/v1/admin/reports/rooms
    #[instrument(level = "debug", skip(self))]
    pub async fn list_room_reports(
        &self,
        admin_user: &UserId,
        room_filter: Option<&RoomId>,
        status_filter: Option<ReportStatus>,
        category_filter: Option<RoomReportCategory>,
        limit: Option<u64>,
        offset: Option<u64>,
    ) -> Result<RoomReportsList> {
        debug!("ðŸ“‹ Listing room reports");

        // Check admin permissions
        if !services().users.is_admin(admin_user)? {
            return Err(Error::BadRequestString(
                ErrorKind::forbidden(),
                "Admin access required".to_string(),
            ));
        }

        let reports = self.reports.read().await;
        let mut filtered_reports: Vec<_> = reports.values()
            .filter(|report| {
                if let Some(room_id) = room_filter {
                    if &report.room_id != room_id {
                        return false;
                    }
                }
                if let Some(status) = &status_filter {
                    if &report.status != status {
                        return false;
                    }
                }
                if let Some(category) = &category_filter {
                    if &report.category != category {
                        return false;
                    }
                }
                true
            })
            .collect();

        // Sort by creation time (newest first)
        filtered_reports.sort_by(|a, b| b.created_at.cmp(&a.created_at));

        let total_count = filtered_reports.len() as u64;
        let offset = offset.unwrap_or(0) as usize;
        let limit = limit.unwrap_or(50) as usize;

        let paginated_reports = filtered_reports
            .into_iter()
            .skip(offset)
            .take(limit)
            .cloned()
            .collect();

        Ok(RoomReportsList {
            reports: paginated_reports,
            total_count,
            offset: offset as u64,
            limit: limit as u64,
        })
    }

    /// Remove reported content (Admin API)
    /// DELETE /_matrix/client/v1/admin/reports/rooms/{report_id}/content
    #[instrument(level = "debug", skip(self))]
    pub async fn remove_reported_content(
        &self,
        admin_user: &UserId,
        report_id: &str,
        reason: Option<String>,
    ) -> Result<()> {
        debug!("ðŸ—‘ï¸ Removing reported content: {}", report_id);

        // Check admin permissions
        if !services().users.is_admin(admin_user)? {
            return Err(Error::BadRequestString(
                ErrorKind::forbidden(),
                "Admin access required".to_string(),
            ));
        }

        let mut reports = self.reports.write().await;
        let report = reports.get_mut(report_id).ok_or_else(|| Error::BadRequestString(
            ErrorKind::NotFound,
            "Report not found".to_string(),
        ))?;

        // Remove content if event ID provided
        if let Some(event_id) = &report.event_id {
            // TODO: Replace with available service methods
            // services().rooms.timeline.remove_event(&report.room_id, event_id)?;
            debug!("ðŸ”§ Would remove event: {} from room: {}", event_id, report.room_id);
            
            // Update metrics
            self.metrics.content_removed.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
        }

        // Update report status
        report.status = ReportStatus::ContentRemoved;
        report.processing_notes.push(ProcessingNote {
            timestamp: SystemTime::now(),
            moderator: admin_user.to_owned(),
            note: reason.unwrap_or_else(|| "Content removed by admin".to_string()),
            note_type: NoteType::Action,
        });

        info!("âœ… Content removed for report: {}", report_id);
        Ok(())
    }

    /// Quarantine room (Admin API)
    /// POST /_matrix/client/v1/admin/reports/rooms/{report_id}/quarantine
    #[instrument(level = "debug", skip(self))]
    pub async fn quarantine_room(
        &self,
        admin_user: &UserId,
        report_id: &str,
        quarantine_settings: QuarantineSettings,
    ) -> Result<()> {
        debug!("ðŸš§ Quarantining room for report: {}", report_id);

        // Check admin permissions
        if !services().users.is_admin(admin_user)? {
            return Err(Error::BadRequestString(
                ErrorKind::forbidden(),
                "Admin access required".to_string(),
            ));
        }

        let mut reports = self.reports.write().await;
        let report = reports.get_mut(report_id).ok_or_else(|| Error::BadRequestString(
            ErrorKind::NotFound,
            "Report not found".to_string(),
        ))?;

        // Implement room quarantine
        self.implement_room_quarantine(&report.room_id, &quarantine_settings).await?;

        // Update report status
        report.status = ReportStatus::RoomQuarantined;
        report.processing_notes.push(ProcessingNote {
            timestamp: SystemTime::now(),
            moderator: admin_user.to_owned(),
            note: format!("Room quarantined: {}", quarantine_settings.reason),
            note_type: NoteType::Action,
        });

        // Update metrics
        self.metrics.rooms_quarantined.fetch_add(1, std::sync::atomic::Ordering::Relaxed);

        info!("âœ… Room quarantined for report: {}", report_id);
        Ok(())
    }

    // ========== Helper Methods ==========

    /// Validate room reporter eligibility
    async fn validate_room_reporter(&self, reporter: &UserId, room_id: &RoomId) -> Result<()> {
        // Check if user exists
        if !services().users.exists(reporter)? {
            return Err(Error::BadRequestString(
                ErrorKind::forbidden(),
                "Reporter user does not exist".to_string(),
            ));
        }

        // Check room membership policy
        let policies = self.moderation_policies.read().await;
        if let Some(policy) = policies.get(room_id) {
            if !policy.reporting_settings.allow_external_reports {
                // Check if user is member
                if !services().rooms.state_cache.is_joined(reporter, room_id)? {
                    return Err(Error::BadRequestString(
                        ErrorKind::forbidden(),
                        "External reports not allowed for this room".to_string(),
                    ));
                }
            }
        }

        Ok(())
    }

    /// Check room reporting rate limits
    async fn check_room_reporting_rate_limits(&self, reporter: &UserId) -> Result<()> {
        let reports = self.reports.read().await;
        let now = SystemTime::now();
        let day_ago = now - Duration::from_secs(24 * 60 * 60);

        let today_reports = reports.values()
            .filter(|report| {
                report.reporter_user_id == reporter && report.created_at > day_ago
            })
            .count() as u32;

        if today_reports >= self.config.max_reports_per_day {
            return Err(Error::BadRequestString(
                ErrorKind::LimitExceeded { retry_after: None },
                "Daily room reporting limit exceeded".to_string(),
            ));
        }

        Ok(())
    }

    /// Get user's role in room
    async fn get_user_room_role(&self, user_id: &UserId, room_id: &RoomId) -> Result<RoomRole> {
        if services().rooms.state_cache.is_joined(user_id, room_id)? {
            // Check if user is room creator
            if let Ok(Some(create_event)) = services().rooms.state_accessor.room_state_get(
                room_id,
                &StateEventType::RoomCreate,
                "",
            ) {
                if create_event.sender == user_id {
                    return Ok(RoomRole::Creator);
                }
            }

            // Check power levels
            // TODO: Replace with available service methods
            // let power_level = services().rooms.state_accessor.user_power_level(user_id, room_id)?;
            let power_level = 0; // Default power level
            match power_level {
                100 => Ok(RoomRole::Admin),
                50..=99 => Ok(RoomRole::Moderator),
                _ => Ok(RoomRole::Member),
            }
        } else {
            Ok(RoomRole::External)
        }
    }

    /// Collect room evidence
    async fn collect_room_evidence(
        &self,
        room_id: &RoomId,
        event_id: Option<&EventId>,
        category: &RoomReportCategory,
    ) -> Result<Vec<ContentEvidence>> {
        let mut evidence = Vec::new();

        // Collect room metadata
        if let Ok(name) = services().rooms.state_accessor.get_name(room_id) {
            let name_str = name.unwrap_or_default();
            let name_bytes = name_str.as_bytes();
            let hash_result = utils::calculate_hash(&[name_bytes]);
            evidence.push(ContentEvidence {
                evidence_type: ContentEvidenceType::RoomMetadata,
                content: serde_json::to_string(&name_str).unwrap_or_default(),
                event_id: None,
                collected_at: SystemTime::now(),
                content_hash: general_purpose::STANDARD.encode(&hash_result),
                metadata: HashMap::new(),
            });
        }

        // Collect specific event evidence
        if let Some(event_id) = event_id {
            if let Ok(Some(event)) = services().rooms.timeline.get_pdu(event_id) {
                let event_str = serde_json::to_string(&event).unwrap_or_default();
                let event_bytes = event_str.as_bytes();
                let hash_result = utils::calculate_hash(&[event_bytes]);
                evidence.push(ContentEvidence {
                    evidence_type: ContentEvidenceType::MessageContent,
                    content: event_str,
                    event_id: Some(event_id.to_owned()),
                    collected_at: SystemTime::now(),
                    content_hash: general_purpose::STANDARD.encode(&hash_result),
                    metadata: HashMap::new(),
                });
            }
        }

        // Collect recent room history for pattern analysis
        if matches!(category, RoomReportCategory::Spam | RoomReportCategory::Harassment) {
            // Get recent events (last 24 hours)
            // TODO: Implement room history collection
        }

        Ok(evidence)
    }

    /// Analyze room content
    async fn analyze_room_content(
        &self,
        _room_id: &RoomId,
        _event_id: Option<&EventId>,
        evidence: &[ContentEvidence],
    ) -> Result<Vec<ContentIssue>> {
        let mut issues = Vec::new();

        // Analyze content patterns
        for evidence_item in evidence {
            if evidence_item.evidence_type == ContentEvidenceType::MessageContent {
                // Simple content analysis - in production, use ML models
                issues.extend(self.analyze_message_content(&evidence_item.content).await?);
            }
        }

        Ok(issues)
    }

    /// Analyze message content for issues
    async fn analyze_message_content(&self, content: &str) -> Result<Vec<ContentIssue>> {
        let mut issues = Vec::new();

        // Simple keyword detection
        let spam_keywords = ["buy now", "click here", "free money", "urgent"];
        let hate_keywords = ["hate", "discrimination", "offensive"];

        let spam_matches = spam_keywords.iter()
            .filter(|&keyword| content.to_lowercase().contains(keyword))
            .count();

        if spam_matches > 0 {
            issues.push(ContentIssue {
                issue_type: "spam_content".to_string(),
                confidence: (spam_matches as f64) / (spam_keywords.len() as f64),
                detected_at: SystemTime::now(),
                details: {
                    let mut details = HashMap::new();
                    details.insert("matches".to_string(), serde_json::Value::Number(
                        serde_json::Number::from(spam_matches)
                    ));
                    details
                },
                suggested_action: Some("Review for spam".to_string()),
            });
        }

        let hate_matches = hate_keywords.iter()
            .filter(|&keyword| content.to_lowercase().contains(keyword))
            .count();

        if hate_matches > 0 {
            issues.push(ContentIssue {
                issue_type: "hate_speech".to_string(),
                confidence: (hate_matches as f64) / (hate_keywords.len() as f64),
                detected_at: SystemTime::now(),
                details: {
                    let mut details = HashMap::new();
                    details.insert("matches".to_string(), serde_json::Value::Number(
                        serde_json::Number::from(hate_matches)
                    ));
                    details
                },
                suggested_action: Some("Manual review required".to_string()),
            });
        }

        Ok(issues)
    }

    /// Calculate room report severity
    async fn calculate_room_report_severity(
        &self,
        category: &RoomReportCategory,
        auto_detected_issues: &[ContentIssue],
    ) -> SeverityLevel {
        // Base severity by category
        let base_severity = match category {
            RoomReportCategory::IllegalContent | RoomReportCategory::IllegalActivity => SeverityLevel::Critical,
            RoomReportCategory::MaliciousContent | RoomReportCategory::HateSpeech => SeverityLevel::High,
            RoomReportCategory::Harassment | RoomReportCategory::InappropriateContent => SeverityLevel::Medium,
            _ => SeverityLevel::Low,
        };

        // Increase severity based on auto-detected issues
        let issue_multiplier = auto_detected_issues.iter()
            .map(|issue| issue.confidence)
            .sum::<f64>();

        if issue_multiplier > 1.5 {
            match base_severity {
                SeverityLevel::Low => SeverityLevel::Medium,
                SeverityLevel::Medium => SeverityLevel::High,
                SeverityLevel::High => SeverityLevel::Critical,
                SeverityLevel::Critical => SeverityLevel::Critical,
            }
        } else {
            base_severity
        }
    }

    /// Auto-process room report
    async fn process_room_report_auto(&self, report_id: &str) -> Result<()> {
        let reports = self.reports.read().await;
        let report = reports.get(report_id).ok_or_else(|| Error::BadRequestString(
            ErrorKind::NotFound,
            "Report not found".to_string(),
        ))?;

        // Check if auto-action is warranted
        if report.severity >= SeverityLevel::High {
            // Check if multiple reports exist for same room
            let similar_reports = reports.values()
                .filter(|r| {
                    r.room_id == report.room_id &&
                    r.category == report.category &&
                    r.created_at > SystemTime::now() - Duration::from_secs(24 * 60 * 60)
                })
                .count();

            if similar_reports >= 3 {
                // Auto-escalate for manual review
                info!("ðŸš¨ Auto-escalating room report: {} (multiple similar reports)", report_id);
            }
        }

        Ok(())
    }

    /// Implement room quarantine
    async fn implement_room_quarantine(
        &self,
        room_id: &RoomId,
        settings: &QuarantineSettings,
    ) -> Result<()> {
        // TODO: Implement actual quarantine logic
        // This would involve:
        // - Preventing new users from joining
        // - Limiting message sending
        // - Hiding room from public directory
        // - Notifying existing members
        
        info!("ðŸš§ Room {} quarantined: {}", room_id, settings.reason);
        Ok(())
    }
}

/// Quarantine settings
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QuarantineSettings {
    /// Quarantine reason
    pub reason: String,
    
    /// Duration of quarantine (None = indefinite)
    pub duration: Option<Duration>,
    
    /// Whether to notify members
    pub notify_members: bool,
    
    /// Whether to allow existing members to continue chatting
    pub allow_existing_chat: bool,
    
    /// Whether to hide from room directory
    pub hide_from_directory: bool,
}

// ========== Response Types ==========

/// Room report status response
#[derive(Debug, Serialize)]
pub struct RoomReportStatus {
    pub report_id: String,
    pub room_id: OwnedRoomId,
    pub status: ReportStatus,
    pub created_at: SystemTime,
    pub last_updated: SystemTime,
    pub category: RoomReportCategory,
    pub severity: SeverityLevel,
}

/// Room reports list response
#[derive(Debug, Serialize)]
pub struct RoomReportsList {
    pub reports: Vec<RoomReport>,
    pub total_count: u64,
    pub offset: u64,
    pub limit: u64,
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_room_reporting_service_creation() {
        let service = RoomReportingService::new();
        assert!(service.config.enabled);
        assert!(service.config.content_analysis);
    }

    #[tokio::test]
    async fn test_room_report_severity_calculation() {
        let service = RoomReportingService::new();
        
        let severity = service.calculate_room_report_severity(
            &RoomReportCategory::IllegalContent,
            &[],
        ).await;
        
        assert_eq!(severity, SeverityLevel::Critical);
    }

    #[tokio::test]
    async fn test_content_analysis() {
        let service = RoomReportingService::new();
        
        let issues = service.analyze_message_content("buy now free money urgent").await.unwrap();
        assert!(!issues.is_empty());
        assert_eq!(issues[0].issue_type, "spam_content");
    }
} 
