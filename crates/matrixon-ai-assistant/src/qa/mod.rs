// =============================================================================
// Matrixon Matrix NextServer - Qa Bot Module
// =============================================================================
//
// Project: Matrixon - Ultra High Performance Matrix NextServer (Synapse Alternative)
// Author: arkSong (arksong2018@gmail.com) - Founder of Matrixon Innovation Project
// Contributors: Matrixon Development Team
// Date: 2024-12-11
// Version: 2.0.0-alpha (PostgreSQL Backend)
// License: Apache 2.0 / MIT
//
// Description:
//   Core business logic service implementation. This module is part of the Matrixon Matrix NextServer
//   implementation, designed for enterprise-grade deployment with 20,000+
//   concurrent connections and <50ms response latency.
//
// Performance Targets:
//   ‚Ä¢ 20k+ concurrent connections
//   ‚Ä¢ <50ms response latency
//   ‚Ä¢ >99% success rate
//   ‚Ä¢ Memory-efficient operation
//   ‚Ä¢ Horizontal scalability
//
// Features:
//   ‚Ä¢ Business logic implementation
//   ‚Ä¢ Service orchestration
//   ‚Ä¢ Event handling and processing
//   ‚Ä¢ State management
//   ‚Ä¢ Enterprise-grade reliability
//
// Architecture:
//   ‚Ä¢ Async/await native implementation
//   ‚Ä¢ Zero-copy operations where possible
//   ‚Ä¢ Memory pool optimization
//   ‚Ä¢ Lock-free data structures
//   ‚Ä¢ Enterprise monitoring integration
//
// Dependencies:
//   ‚Ä¢ Tokio async runtime
//   ‚Ä¢ Structured logging with tracing
//   ‚Ä¢ Error handling with anyhow/thiserror
//   ‚Ä¢ Serialization with serde
//   ‚Ä¢ Matrix protocol types with ruma
//
// References:
//   ‚Ä¢ Matrix.org specification: https://matrix.org/
//   ‚Ä¢ Synapse reference: https://github.com/element-hq/synapse
//   ‚Ä¢ Matrix spec: https://spec.matrix.org/
//   ‚Ä¢ Performance guidelines: Internal Matrixon documentation
//
// Quality Assurance:
//   ‚Ä¢ Comprehensive unit testing
//   ‚Ä¢ Integration test coverage
//   ‚Ä¢ Performance benchmarking
//   ‚Ä¢ Memory leak detection
//   ‚Ä¢ Security audit compliance
//
// =============================================================================

use std::{
    collections::HashMap,
    sync::Arc,
    time::{Duration, Instant},
    path::Path,
};

use tokio::{
    sync::RwLock,
};

use tracing::{debug, info, instrument, warn};
use serde::{Deserialize, Serialize};

use crate::{Error, Result};
use super::{
    llm_integration::{LlmIntegration, LlmRequest},
    QaBotConfig,
};

/// Q&A request structure
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QaRequest {
    pub question: String,
    pub user_id: String,
    pub room_id: Option<String>,
    pub language: Option<String>,
    pub context: Option<HashMap<String, String>>,
}

/// Q&A response structure
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QaResponse {
    pub answer: String,
    pub metadata: HashMap<String, String>,
    pub confidence: f32,
    pub processing_time_ms: u64,
    pub sources: Vec<KnowledgeSource>,
    pub suggested_followup: Vec<String>,
    pub category: String,
}

/// Knowledge source reference
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct KnowledgeSource {
    pub title: String,
    pub url: Option<String>,
    pub excerpt: String,
    pub relevance_score: f32,
}

/// Knowledge base entry
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct KnowledgeEntry {
    pub id: String,
    pub title: String,
    pub content: String,
    pub category: String,
    pub tags: Vec<String>,
    pub language: String,
    pub last_updated: u64,
    pub keywords: Vec<String>,
}

/// Q&A statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QaStats {
    pub total_questions: u64,
    pub successful_answers: u64,
    pub failed_answers: u64,
    pub average_confidence: f32,
    pub average_response_time_ms: f64,
    pub top_categories: HashMap<String, u32>,
    pub knowledge_base_size: usize,
}

/// Q&A Bot Service
pub struct QaBot {
    /// Configuration
    config: QaBotConfig,
    
    /// LLM integration for generating answers
    llm_integration: Arc<LlmIntegration>,
    
    /// Knowledge base
    knowledge_base: Arc<RwLock<HashMap<String, KnowledgeEntry>>>,
    
    /// Q&A cache
    qa_cache: Arc<RwLock<HashMap<String, (QaResponse, Instant)>>>,
    
    /// Statistics tracking
    stats: Arc<RwLock<QaStats>>,
}

impl QaBot {
    /// Create new Q&A bot
    #[instrument(level = "debug")]
    pub async fn new(
        config: QaBotConfig,
        llm_integration: Arc<LlmIntegration>,
    ) -> Result<Self> {
        let start = Instant::now();
        info!("üîß Initializing Q&A Bot");

        let knowledge_base = Arc::new(RwLock::new(HashMap::new()));
        let qa_cache = Arc::new(RwLock::new(HashMap::new()));
        let stats = Arc::new(RwLock::new(QaStats {
            total_questions: 0,
            successful_answers: 0,
            failed_answers: 0,
            average_confidence: 0.0,
            average_response_time_ms: 0.0,
            top_categories: HashMap::new(),
            knowledge_base_size: 0,
        }));

        let bot = Self {
            config: config.clone(),
            llm_integration,
            knowledge_base: knowledge_base.clone(),
            qa_cache,
            stats,
        };

        // Load knowledge base if configured
        if let Some(kb_path) = &config.knowledge_base_path {
            bot.load_knowledge_base(kb_path).await?;
        } else {
            // Initialize with default Matrix/matrixon knowledge
            bot.initialize_default_knowledge().await?;
        }

        info!("‚úÖ Q&A Bot initialized in {:?}", start.elapsed());
        Ok(bot)
    }

    /// Answer a question
    #[instrument(level = "debug", skip(self, request))]
    pub async fn answer_question(&self, request: &QaRequest) -> Result<QaResponse> {
        let start = Instant::now();
        debug!("üîß Answering question: {}", request.question.chars().take(50).collect::<String>());

        if !self.config.enabled {
            return Err(Error::BadRequest("Q&A bot is not enabled".to_string()));
        }

        // Update statistics
        {
            let mut stats = self.stats.write().await;
            stats.total_questions += 1;
        }

        // Check cache first
        let cache_key = self.generate_cache_key(request);
        if let Some(cached_answer) = self.get_cached_answer(&cache_key).await {
            debug!("üìã Returning cached Q&A response");
            return Ok(cached_answer);
        }

        // Search knowledge base
        let relevant_entries = self.search_knowledge_base(&request.question, request.language.as_deref()).await?;
        
        // Determine question category
        let category = self.categorize_question(&request.question).await;
        
        // Generate answer using LLM with knowledge context
        let answer_text = self.generate_answer_with_context(request, &relevant_entries).await?;
        
        // Extract sources and confidence
        let sources = self.extract_sources(&relevant_entries);
        let confidence = self.calculate_confidence(&relevant_entries, &answer_text);
        
        // Generate follow-up suggestions
        let suggested_followup = self.generate_followup_suggestions(&request.question, &category).await;

        // Build metadata
        let mut metadata = HashMap::new();
        metadata.insert("category".to_string(), category.clone());
        metadata.insert("language".to_string(), request.language.clone().unwrap_or_else(|| "en".to_string()));
        metadata.insert("knowledge_entries_used".to_string(), relevant_entries.len().to_string());

        let response = QaResponse {
            answer: answer_text.to_string(),
            metadata,
            confidence,
            processing_time_ms: start.elapsed().as_millis() as u64,
            sources,
            suggested_followup,
            category: category.clone(),
        };

        // Cache the response
        self.cache_answer(&cache_key, &response).await;

        // Update success statistics
        {
            let mut stats = self.stats.write().await;
            stats.successful_answers += 1;
            stats.average_response_time_ms = 
                (stats.average_response_time_ms * (stats.successful_answers - 1) as f64 + 
                 response.processing_time_ms as f64) / stats.successful_answers as f64;
            stats.average_confidence = 
                (stats.average_confidence * (stats.successful_answers - 1) as f32 + 
                 response.confidence) / stats.successful_answers as f32;
            
            // Update category statistics
            let count = stats.top_categories.entry(category).or_insert(0);
            *count += 1;
        }

        info!("‚úÖ Question answered in {:?}", start.elapsed());
        Ok(response)
    }

    /// Search knowledge base for relevant entries
    async fn search_knowledge_base(&self, question: &str, language: Option<&str>) -> Result<Vec<KnowledgeEntry>> {
        debug!("üîç Searching knowledge base for: {}", question.chars().take(30).collect::<String>());

        let knowledge_base = self.knowledge_base.read().await;
        let mut relevant_entries = Vec::new();
        let question_lower = question.to_lowercase();

        for entry in knowledge_base.values() {
            // Language filter
            if let Some(lang) = language {
                if entry.language != lang && entry.language != "en" { // Always include English as fallback
                    continue;
                }
            }

            // Calculate relevance score
            let mut relevance_score = 0.0;

            // Title matching (higher weight)
            if entry.title.to_lowercase().contains(&question_lower) {
                relevance_score += 0.4;
            }

            // Keyword matching
            for keyword in &entry.keywords {
                if question_lower.contains(&keyword.to_lowercase()) {
                    relevance_score += 0.2;
                }
            }

            // Content matching (lower weight due to potential noise)
            if entry.content.to_lowercase().contains(&question_lower) {
                relevance_score += 0.1;
            }

            // Tag matching
            for tag in &entry.tags {
                if question_lower.contains(&tag.to_lowercase()) {
                    relevance_score += 0.15;
                }
            }

            // Category matching
            if question_lower.contains(&entry.category.to_lowercase()) {
                relevance_score += 0.1;
            }

            if relevance_score > 0.1 { // Minimum relevance threshold
                relevant_entries.push((relevance_score, entry.clone()));
            }
        }

        // Sort by relevance and take top results
        relevant_entries.sort_by(|a, b| b.0.partial_cmp(&a.0).unwrap_or(std::cmp::Ordering::Equal));
        let top_entries: Vec<KnowledgeEntry> = relevant_entries
            .into_iter()
            .take(5) // Top 5 most relevant entries
            .map(|(_, entry)| entry)
            .collect();

        debug!("üìä Found {} relevant knowledge entries", top_entries.len());
        Ok(top_entries)
    }

    /// Generate answer using LLM with knowledge context
    async fn generate_answer_with_context(
        &self,
        request: &QaRequest,
        knowledge_entries: &[KnowledgeEntry],
    ) -> Result<String> {
        debug!("ü§ñ Generating answer with LLM");

        let knowledge_context = if knowledge_entries.is_empty() {
            "No specific documentation found. Please provide a general helpful response.".to_string()
        } else {
            knowledge_entries
                .iter()
                .map(|entry| format!("Title: {}\nContent: {}", entry.title, entry.content))
                .collect::<Vec<_>>()
                .join("\n\n")
        };

        let system_prompt = format!(
            "You are matrixon AI Assistant, a helpful assistant for the matrixon Matrix NextServer. \
             Answer the user's question accurately and helpfully based on the provided documentation. \
             If the documentation doesn't contain the answer, provide general Matrix/NextServer guidance. \
             Be concise but thorough. Language preference: {}\n\n\
             Available Documentation:\n{}",
            request.language.as_ref().unwrap_or(&"en".to_string()),
            knowledge_context
        );

        let user_prompt = format!(
            "Question: {}\n\n\
             Please provide a helpful and accurate answer. If you need to refer to external resources, \
             mention them appropriately. Focus on practical solutions when possible.",
            request.question
        );

        // Add context if provided
        let enhanced_prompt = if let Some(context) = &request.context {
            let context_str = context
                .iter()
                .map(|(k, v)| format!("{}: {}", k, v))
                .collect::<Vec<_>>()
                .join(", ");
            format!("{}\n\nAdditional context: {}", user_prompt, context_str)
        } else {
            user_prompt
        };

        // Prepare LLM request
        let llm_request = LlmRequest {
            model: "gpt4o".to_string(), // Use configured default
            messages: vec![
                HashMap::from([
                    ("role".to_string(), "system".to_string()),
                    ("content".to_string(), system_prompt),
                ]),
                HashMap::from([
                    ("role".to_string(), "user".to_string()),
                    ("content".to_string(), enhanced_prompt),
                ]),
            ],
            max_tokens: Some(self.config.max_context_length),
            temperature: Some(0.2), // Lower temperature for factual responses
            top_p: Some(0.9),
            user_id: Some(request.user_id.clone()),
        };

        let llm_response = self.llm_integration.generate_response(&llm_request).await?;
        Ok(llm_response.content)
    }

    /// Categorize question into predefined categories
    async fn categorize_question(&self, question: &str) -> String {
        let question_lower = question.to_lowercase();

        if question_lower.contains("install") || question_lower.contains("setup") || question_lower.contains("configuration") {
            "Installation & Setup".to_string()
        } else if question_lower.contains("matrix") || question_lower.contains("protocol") || question_lower.contains("federation") {
            "Matrix Protocol".to_string()
        } else if question_lower.contains("room") || question_lower.contains("channel") || question_lower.contains("join") {
            "Room Management".to_string()
        } else if question_lower.contains("user") || question_lower.contains("account") || question_lower.contains("permission") {
            "User Management".to_string()
        } else if question_lower.contains("error") || question_lower.contains("problem") || question_lower.contains("issue") {
            "Troubleshooting".to_string()
        } else if question_lower.contains("api") || question_lower.contains("development") || question_lower.contains("code") {
            "Development".to_string()
        } else if question_lower.contains("security") || question_lower.contains("encryption") || question_lower.contains("privacy") {
            "Security".to_string()
        } else {
            "General".to_string()
        }
    }

    /// Extract sources from knowledge entries
    fn extract_sources(&self, entries: &[KnowledgeEntry]) -> Vec<KnowledgeSource> {
        entries
            .iter()
            .map(|entry| KnowledgeSource {
                title: entry.title.clone(),
                url: None, // Could be extended to include URLs
                excerpt: entry.content.chars().take(200).collect::<String>() + "...",
                relevance_score: 0.8, // Simplified relevance score
            })
            .collect()
    }

    /// Calculate confidence score
    fn calculate_confidence(&self, entries: &[KnowledgeEntry], _answer: &str) -> f32 {
        if entries.is_empty() {
            0.3 // Low confidence without knowledge base support
        } else if entries.len() >= 3 {
            0.9 // High confidence with multiple supporting entries
        } else if entries.len() >= 2 {
            0.8 // Good confidence with some supporting entries
        } else {
            0.6 // Moderate confidence with limited support
        }
    }

    /// Generate follow-up suggestions
    async fn generate_followup_suggestions(&self, _question: &str, category: &str) -> Vec<String> {
        match category {
            "Installation & Setup" => vec![
                "How do I configure matrixon for production?".to_string(),
                "What are the system requirements?".to_string(),
                "How do I enable federation?".to_string(),
            ],
            "Matrix Protocol" => vec![
                "How does Matrix federation work?".to_string(),
                "What are Matrix events?".to_string(),
                "How do I use the Matrix API?".to_string(),
            ],
            "Room Management" => vec![
                "How do I create a private room?".to_string(),
                "How do I manage room permissions?".to_string(),
                "How do I invite users to a room?".to_string(),
            ],
            "Troubleshooting" => vec![
                "How do I check matrixon logs?".to_string(),
                "What are common configuration issues?".to_string(),
                "How do I debug federation problems?".to_string(),
            ],
            _ => vec![
                "What is matrixon?".to_string(),
                "How do I get started with Matrix?".to_string(),
                "Where can I find more documentation?".to_string(),
            ],
        }
    }

    /// Load knowledge base from file system
    async fn load_knowledge_base(&self, path: &str) -> Result<()> {
        debug!("üìö Loading knowledge base from: {}", path);

        let path = Path::new(path);
        if !path.exists() {
            warn!("Knowledge base path does not exist: {}", path.display());
            return self.initialize_default_knowledge().await;
        }

        // For now, initialize with default knowledge
        // In production, this would load from actual files
        self.initialize_default_knowledge().await
    }

    /// Initialize default knowledge base
    async fn initialize_default_knowledge(&self) -> Result<()> {
        debug!("üìö Initializing default knowledge base");

        let mut knowledge_base = self.knowledge_base.write().await;
        
        // Matrix Protocol basics
        knowledge_base.insert("matrix_intro".to_string(), KnowledgeEntry {
            id: "matrix_intro".to_string(),
            title: "What is Matrix?".to_string(),
            content: "Matrix is an open standard for interoperable, decentralised, real-time communication over IP. It provides HTTP APIs and open source reference implementations for creating and running your own real-time communication infrastructure.".to_string(),
            category: "Matrix Protocol".to_string(),
            tags: vec!["matrix".to_string(), "protocol".to_string(), "introduction".to_string()],
            language: "en".to_string(),
            last_updated: 1703001600000,
            keywords: vec!["matrix".to_string(), "protocol".to_string(), "communication".to_string(), "decentralized".to_string()],
        });

        // matrixon introduction
        knowledge_base.insert("matrixon_intro".to_string(), KnowledgeEntry {
            id: "matrixon_intro".to_string(),
            title: "What is matrixon?".to_string(),
            content: "matrixon is a Matrix NextServer implementation written in Rust. It aims to be fast, memory-efficient, and easy to set up and maintain. matrixon supports most Matrix features including federation, end-to-end encryption, and the full client-server API.".to_string(),
            category: "Installation & Setup".to_string(),
            tags: vec!["matrixon".to_string(), "NextServer".to_string(), "rust".to_string()],
            language: "en".to_string(),
            last_updated: 1703001600000,
            keywords: vec!["matrixon".to_string(), "NextServer".to_string(), "matrix".to_string(), "rust".to_string()],
        });

        // Installation guide
        knowledge_base.insert("installation".to_string(), KnowledgeEntry {
            id: "installation".to_string(),
            title: "How to install matrixon".to_string(),
            content: "To install matrixon: 1. Download the binary from GitHub releases or compile from source. 2. Create a configuration file (matrixon.toml). 3. Configure your domain and server settings. 4. Set up a reverse proxy (nginx/Apache). 5. Start the matrixon service. Make sure to configure federation properly for multi-server communication.".to_string(),
            category: "Installation & Setup".to_string(),
            tags: vec!["installation".to_string(), "setup".to_string(), "configuration".to_string()],
            language: "en".to_string(),
            last_updated: 1703001600000,
            keywords: vec!["install".to_string(), "setup".to_string(), "configuration".to_string(), "nginx".to_string()],
        });

        // Room management
        knowledge_base.insert("rooms".to_string(), KnowledgeEntry {
            id: "rooms".to_string(),
            title: "Matrix Rooms and Spaces".to_string(),
            content: "Matrix rooms are virtual spaces where users can send messages, share files, and communicate. Rooms can be public (anyone can join) or private (invite-only). Spaces are special rooms that contain other rooms, helping organize related discussions. Room aliases make rooms easier to find and share.".to_string(),
            category: "Room Management".to_string(),
            tags: vec!["rooms".to_string(), "spaces".to_string(), "chat".to_string()],
            language: "en".to_string(),
            last_updated: 1703001600000,
            keywords: vec!["room".to_string(), "space".to_string(), "chat".to_string(), "communication".to_string()],
        });

        // Federation
        knowledge_base.insert("federation".to_string(), KnowledgeEntry {
            id: "federation".to_string(),
            title: "Matrix Federation".to_string(),
            content: "Federation allows different Matrix NextServers to communicate with each other, creating a global network. Users on different servers can join the same rooms and communicate seamlessly. Federation requires proper DNS configuration and SSL certificates. The server-to-server API handles federated communication.".to_string(),
            category: "Matrix Protocol".to_string(),
            tags: vec!["federation".to_string(), "servers".to_string(), "network".to_string()],
            language: "en".to_string(),
            last_updated: 1703001600000,
            keywords: vec!["federation".to_string(), "server".to_string(), "network".to_string(), "communication".to_string()],
        });

        // Update statistics
        {
            let mut stats = self.stats.write().await;
            stats.knowledge_base_size = knowledge_base.len();
        }

        info!("‚úÖ Initialized knowledge base with {} entries", knowledge_base.len());
        Ok(())
    }

    /// Generate cache key for Q&A request
    fn generate_cache_key(&self, request: &QaRequest) -> String {
        let request_hash = md5::compute(format!("{:?}", request));
        format!("qa_{:x}", request_hash)
    }

    /// Get cached answer
    async fn get_cached_answer(&self, cache_key: &str) -> Option<QaResponse> {
        let cache = self.qa_cache.read().await;
        if let Some((answer, timestamp)) = cache.get(cache_key) {
            if timestamp.elapsed().as_secs() < 3600 { // 1 hour cache TTL
                return Some(answer.clone());
            }
        }
        None
    }

    /// Cache Q&A response
    async fn cache_answer(&self, cache_key: &str, answer: &QaResponse) {
        let mut cache = self.qa_cache.write().await;
        cache.insert(cache_key.to_string(), (answer.clone(), Instant::now()));
        
        // Clean up old entries
        if cache.len() > 500 {
            let cutoff = Instant::now() - Duration::from_secs(3600);
            cache.retain(|_, (_, timestamp)| *timestamp > cutoff);
        }
    }

    /// Get Q&A statistics
    #[instrument(level = "debug", skip(self))]
    pub async fn get_statistics(&self) -> QaStats {
        let mut stats = self.stats.read().await.clone();
        
        // Update knowledge base size
        let knowledge_base = self.knowledge_base.read().await;
        stats.knowledge_base_size = knowledge_base.len();

        stats
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::config::QaBotConfig;
    use crate::llm_integration::{LlmProviderConfig, LlmProvider};
    use std::time::Duration;
    use std::sync::Arc;

    #[tokio::test]
    async fn test_qa_bot() {
        let config = QaBotConfig {
            enabled: true,
            model_name: "gpt-3.5-turbo".to_string(),
            max_context_length: 4000,
            max_response_length: 1000,
            temperature: 0.7,
            top_p: 1.0,
            cache_size: 1000,
            cache_ttl_seconds: 3600,
            rate_limit_per_minute: 60,
            knowledge_base_path: None,
            supported_languages: vec!["en".to_string()],
            confidence_threshold: 0.7,
        };

        // Create mock LLM integration
        let mut providers = std::collections::HashMap::new();
        providers.insert("test".to_string(), LlmProviderConfig {
            provider_type: LlmProvider::Custom,
            api_endpoint: Some("http://localhost:8080".to_string()),
            api_key: None,
            model_name: "test-model".to_string(),
            max_tokens: 1000,
            temperature: 0.7,
            top_p: 0.9,
            rate_limit_per_minute: 60,
        });

        let llm_integration = Arc::new(LlmIntegration::new(providers).await.unwrap());
        let qa_bot = QaBot::new(config, llm_integration).await.unwrap();
        
        // Test knowledge base operations
        let entry = KnowledgeEntry {
            id: "test_entry".to_string(),
            title: "Test Entry".to_string(),
            content: "Test content".to_string(),
            category: "Test".to_string(),
            tags: vec!["test".to_string()],
            language: "en".to_string(),
            last_updated: 1703001600000,
            keywords: vec!["test".to_string()],
        };

        // Test adding and removing knowledge entries
        qa_bot.knowledge_base.write().await.insert(entry.id.clone(), entry.clone());
        qa_bot.knowledge_base.write().await.remove(&entry.id);
    }
}
